{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dafc54d8",
   "metadata": {},
   "source": [
    "# TED web scraper\n",
    "\n",
    "The purpose of this notebook is to create a web scraper that collects TED talks:\n",
    "- slugs (url part) by topic\n",
    "- info (talk and speaker details, including transcripts) by slugs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cefc8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab7afa6",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Downloding slug(s) by topic\n",
    "slug = url element to a talk\n",
    "\n",
    "There are options available:<br>\n",
    "A. **Version 1** - using api request<br> \n",
    "B. **Version 2** - parsing html using beautiful soup -> might not work everytime<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1708cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Inputs\n",
    "\n",
    "topic='machine learning' # Select a topic from the list available here: https://www.ted.com/topics\n",
    "\n",
    "filename=f\"slug/TED_Talk_{topic}_URLs.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbbcba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1 - using api request\n",
    "\n",
    "def api_scraping(topic, filename):\n",
    "    urls = []\n",
    "    page_number=0\n",
    "    total_pages=0\n",
    "    f = open(filename, 'w')\n",
    "    \n",
    "    while True:\n",
    "        # The API endpoint to request\n",
    "        endpoint = \"https://zenith-prod-alt.ted.com/api/search\"\n",
    "\n",
    "        # The headers for the request\n",
    "        headers = {\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "      }\n",
    "\n",
    "      # The data for the request\n",
    "        data = [{\n",
    "              \"indexName\": \"coyote_models_acme_videos_alias_21e1372f285984be956cd03b7ad3406e\",\n",
    "              \"params\": {\n",
    "                  \"attributeForDistinct\": \"objectID\",\n",
    "                  \"distinct\": 1,\n",
    "                  \"facetFilters\": [\n",
    "                      [\n",
    "                          \"tags:\"+topic\n",
    "                      ]\n",
    "                  ],\n",
    "                  \"facets\": [\n",
    "                      \"subtitle_languages\",\n",
    "                      \"tags\"\n",
    "                  ],\n",
    "                  \"highlightPostTag\": \"__/ais-highlight__\",\n",
    "                  \"highlightPreTag\": \"__ais-highlight__\",\n",
    "                  \"hitsPerPage\": 24,\n",
    "                  \"maxValuesPerFacet\": 500,\n",
    "                  \"page\": page_number,\n",
    "                  \"query\": \"\",\n",
    "                  \"tagFilters\": \"\"\n",
    "              }\n",
    "          },\n",
    "          {\n",
    "              \"indexName\": \"coyote_models_acme_videos_alias_21e1372f285984be956cd03b7ad3406e\",\n",
    "              \"params\": {\n",
    "                  \"analytics\": False,\n",
    "                  \"attributeForDistinct\": \"objectID\",\n",
    "                  \"clickAnalytics\": False,\n",
    "                  \"distinct\": 1,\n",
    "                  \"facets\": \"tags\",\n",
    "                  \"highlightPostTag\": \"__/ais-highlight__\",\n",
    "                  \"highlightPreTag\": \"__ais-highlight__\",\n",
    "                  \"hitsPerPage\": 0,\n",
    "                  \"maxValuesPerFacet\": 500,\n",
    "                  \"page\": page_number,\n",
    "                  \"query\": \"\"\n",
    "              }\n",
    "          }]\n",
    "\n",
    "\n",
    "\n",
    "        # Send the POST request and get the response\n",
    "        response = requests.post(endpoint, headers=headers, data=json.dumps(data))\n",
    "        if page_number==0:\n",
    "            total_pages=response.json()['results'][0]['nbPages']\n",
    "\n",
    "        for slug in response.json()['results'][0]['hits']:\n",
    "            urls.append(slug['slug'])\n",
    "        print(f\"{page_number} / {total_pages}\")\n",
    "        page_number+=1\n",
    "        if page_number>total_pages:\n",
    "            break\n",
    "    f.write('\\n'.join(urls))\n",
    "    f.close()\n",
    "\n",
    "    print(f\"Done.{len(urls)} URLs for topic {topic} have been saved in {f}.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "216da8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2 - using beautiful soup parsing\n",
    "\n",
    "def html_scrapping(topic, filename):\n",
    "    urls = []\n",
    "    page_number=1\n",
    "    f = open(filename, 'w')\n",
    "    \n",
    "    while True:\n",
    "        res  =  requests.get(\"https://www.ted.com/talks?page=\"+str(page_number)+\"&sort=newest&topics[]=\"+topic)\n",
    "\n",
    "        soup = BeautifulSoup(res.text)\n",
    "        e=soup.find('div', id='browse-results')\n",
    "\n",
    "        string_end=\"Sorry. We couldn't find a talk quite like that.\" \n",
    "\n",
    "        if e is None:\n",
    "            break\n",
    "        elif e.find('div', class_='h3 m2') is not None:\n",
    "            if e.find('div', class_='h3 m2').text==string_end:\n",
    "                break\n",
    "        else:\n",
    "            for elem in e.find_all(\"h4\", class_=\"h9 m5 f-w:700\"):\n",
    "                urls.append(elem.find('a')['href'])\n",
    "            page_number+=1\n",
    "\n",
    "\n",
    "    f.write('\\n'.join(urls))\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "127d237a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 4\n",
      "1 / 4\n",
      "2 / 4\n",
      "3 / 4\n",
      "4 / 4\n",
      "Done.79 URLs for topic machine learning have been saved in <_io.TextIOWrapper name='slug/TED_Talk_machine learning_URLs.txt' mode='w' encoding='cp1252'>.\n"
     ]
    }
   ],
   "source": [
    "api_scraping(topic,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7086b0d3",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Downloading content for each talk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87a5deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "\n",
    "# function to get id\n",
    "\n",
    "def getBuildID():\n",
    "    response=requests.get(\"https://ted.com\")\n",
    "    buildID=str(response.content).split(\"buildId\\\":\\\"\")[1].split(\"\\\"\")[0]\n",
    "    return buildID\n",
    "\n",
    "# function to build url\n",
    "\n",
    "def buildDataURL(slug):\n",
    "    daily_id=getBuildID()\n",
    "    base=f\"https://www.ted.com/_next/data/{daily_id}/talks/\"\n",
    "    mid=\".json?slug=\"\n",
    "    url=base+slug+mid+slug\n",
    "    return url\n",
    "\n",
    "# function to get slug data\n",
    "issues={}\n",
    "\n",
    "def getSlugData(url):\n",
    "    response=requests.get(url)\n",
    "    try:\n",
    "        return response.json()\n",
    "    except ValueError:\n",
    "        print(url)\n",
    "        print(response)\n",
    "        issues[url]=response\n",
    "\n",
    "# function to export json data for a list of slugs\n",
    "\n",
    "def extract_json_by_slug(path_to_json,slugs, retry_limit=3):\n",
    "    count=0\n",
    "    max_count=10\n",
    "\n",
    "    data_list=[]\n",
    "    slugs_retry={}\n",
    "\n",
    "    for i, slug in enumerate(slugs):\n",
    "        count+=1\n",
    "        slugURL = buildDataURL(slug)\n",
    "        data_json = getSlugData(slugURL)\n",
    "\n",
    "        if data_json is None: #checking if the respose is null\n",
    "            if slug in slugs_retry:\n",
    "                slugs_retry[slug] += 1\n",
    "            else:\n",
    "                slugs_retry[slug] = 1\n",
    "        else:\n",
    "            json_object = json.dumps(data_json)\n",
    "            file_slug=slug if len(slug)<100 else slug[:101]\n",
    "            \n",
    "            with open(f\"{path_to_json}{file_slug}.json\", \"w\") as outfile:\n",
    "                outfile.write(json_object)\n",
    "\n",
    "        # adding throttling/wait time to stay within rate rate limiting parameters     \n",
    "        if count>max_count:\n",
    "            time.sleep(10)\n",
    "            count=0\n",
    "        else:\n",
    "            time.sleep(3)\n",
    "        \n",
    "        # Print progress for every 100 slugs processed\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Processed {i+1} out of {len(slugs)} slugs.\")\n",
    "    \n",
    "    # Retry slugs that failed but haven't reached the retry limit\n",
    "    slugs_to_retry = [slug for slug, retries in slugs_retry.items() if retries < retry_limit]\n",
    "    \n",
    "    print(f\"There are {len(slugs_to_retry)} slugs that still return None after retries.\")\n",
    "    \n",
    "    if len(slugs_to_retry)!=0:\n",
    "        extract_json_by_slug(slugs_to_retry, retry_limit)\n",
    "        \n",
    "    if len(issues) > 0:\n",
    "        print(issues)\n",
    "    return slugs_to_retry,issues\n",
    "\n",
    "\n",
    "# Data quality check: function to check for null jsons and missing transcripts\n",
    "missing=[]\n",
    "nulls=[]\n",
    "\n",
    "def check_null_missing_transcript(path_to_json):\n",
    "    count_missing=0\n",
    "    count_null=0\n",
    "    \n",
    "    json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "    for jfile in json_files:\n",
    "        with open(os.path.join(path_to_json, jfile)) as json_file:\n",
    "            json_text = json.load(json_file)\n",
    "            if json_text is None:\n",
    "                count_null+=1\n",
    "                nulls.append(jfile)\n",
    "            elif json_text[\"pageProps\"][\"transcriptData\"][\"translation\"] is None:\n",
    "                count_missing+=1\n",
    "                missing.append(jfile)\n",
    "    print(\"Summary:\")        \n",
    "    print(f\"There are {count_null} null jsons files\")\n",
    "    print(f\"There are {count_missing} jsons files with missing transcripts\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c2f90e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TED_Talk_Ai_URLs.txt\n",
      "144 slugs found\n",
      "TED_Talk_computers_URLs.txt\n",
      "213 slugs found\n",
      "TED_Talk_Data_URLs.txt\n",
      "207 slugs found\n",
      "TED_Talk_machine learning_URLs.txt\n",
      "79 slugs found\n",
      "TED_Talk_technology_URLs.txt\n",
      "1346 slugs found\n"
     ]
    }
   ],
   "source": [
    "# define which slugs to be loaded and where to save json files\n",
    "path_to_json = 'jsons/'\n",
    "path_to_slug = 'slug/'\n",
    "slug_files = [pos_slug for pos_slug in os.listdir(path_to_slug) if pos_slug.endswith('.txt')]\n",
    "\n",
    "slugs_combined=[]\n",
    "\n",
    "for sfile in slug_files:\n",
    "    print(sfile)\n",
    "    with open(os.path.join(path_to_slug, sfile)) as slug_file:\n",
    "        loaded=slug_file.read().splitlines()\n",
    "        print(f\"{len(loaded)} slugs found\")\n",
    "        slugs_combined.append(loaded)\n",
    "\n",
    "# combining all slugs and removing duplicates        \n",
    "slugs = list(set([item for sublist in slugs_combined for item in sublist]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7824add1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1364\n"
     ]
    }
   ],
   "source": [
    "print(len(slugs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eebd955b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 out of 1364 slugs.\n",
      "Processed 200 out of 1364 slugs.\n",
      "Processed 300 out of 1364 slugs.\n",
      "Processed 400 out of 1364 slugs.\n",
      "Processed 500 out of 1364 slugs.\n",
      "<Response [429]>\n",
      "Processed 600 out of 1364 slugs.\n",
      "Processed 700 out of 1364 slugs.\n",
      "Processed 800 out of 1364 slugs.\n",
      "Processed 900 out of 1364 slugs.\n",
      "Processed 1000 out of 1364 slugs.\n",
      "Processed 1100 out of 1364 slugs.\n",
      "Processed 1200 out of 1364 slugs.\n",
      "Processed 1300 out of 1364 slugs.\n",
      "There are 1 slugs that still return None after retries.\n",
      "There are 0 slugs that still return None after retries.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['balsher_singh_sidhu_are_we_running_out_of_clean_water']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_json_by_slug(path_to_json,slugs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2865fcf4",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Checking for missing transcripts & null json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1110e3db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "There are 0 null jsons files\n",
      "There are 133 jsons files with missing transcripts\n"
     ]
    }
   ],
   "source": [
    "check_null_missing_transcript(path_to_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9bbf345d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['achin_bhowmik_interactive_experiences_with_perceptual_computing.json', 'adam_cutler_can_we_be_friends_with_our_ai.json', 'alan_amling_the_future_of_delivery_in_our_new_on_demand_economy.json', 'alastair_o_neill_when_genetic_choice_becomes_personal.json', 'alison_sander_megatrends_5_tips_on_the_art_and_science_of_trend_tracking.json', 'andrew_arruda_the_world_s_first_ai_legal_assistant.json', 'anil_raj_micro_power_plants_to_bring_electricity_to_millions.json', 'arindam_bhattacharya_globalization_isn_t_declining_it_s_transforming.json', 'ayanna_howard_why_we_need_to_build_robots_we_can_trust.json', 'a_dara_dotz_an_ingenious_solution_for_aid_in_disaster_zones.json', 'baroness_beeban_kidron_why_we_must_protect_the_digital_rights_of_children.json', 'ben_pring_the_new_jobs_of_the_future_and_other_insights_on_the_changing_workforce.json', 'bruno_michel_how_our_brains_will_keep_up_with_ai.json', 'bryan_kramer_why_social_media_is_reimagining_our_future.json', 'carey_kolaja_a_vision_for_truly_secure_and_seamless_transactions.json', 'carl_vause_meet_the_robot_designed_like_an_octopus_tentacle.json', 'cesar_hidalgo_una_idea_osada_para_reemplazar_a_los_politicos.json', 'christian_bailey_the_joys_of_amateur_aviation.json', 'christian_schroter_how_we_re_tracking_medical_donations_across_the_world.json', 'christina_balch_selfies_and_seeing_ourselves_one_artist_s_look_in_the_mirror.json', 'cindy_gallop_make_love_not_porn.json', 'craig_silliman_how_covid_19_impacted_a_global_tech_company_and_spurred_innovation.json', 'crystal_martin_small_ways_women_can_support_each_other_at_work_jan_2017.json', 'dan_walker_can_we_print_smart_objects.json', 'dario_gil_the_future_of_expertise.json', 'david_carroll_how_i_sued_cambridge_analytica_over_my_personal_data.json', 'del_harvey_protecting_privacy_at_twitter.json', 'derek_banta_what_if_our_data_could_be_protected_online.json', 'drew_humphreys_how_machine_learning_can_teach_us_to_build_more_effective_teams.json', 'drew_silverstein_how_technology_can_democratize_music.json', 'emmanuel_schanzer_why_is_algebra_so_hard_the_answer_is_surprisingly_simple.json', 'erica_joy_baker_how_do_we_bridge_the_anxiety_gap_at_work.json', 'erick_brethenoux_the_big_data_behind_complex_human_emotions.json', 'eric_mibuari_can_credit_scores_be_determined_by_cellphone_data.json', 'eric_yuan_how_to_connect_while_apart.json', 'florian_pinel_how_ibm_s_watson_is_redefining_the_future_of_cooking.json', 'francois_candelon_how_do_we_stop_the_massive_inequality_dominating_the_world_economy.json', 'frans_von_der_dunk_the_importance_of_space_lawyers.json', 'galit_ariel_how_ar_can_make_us_feel_more_connected_to_the_world.json', 'gary_niekerk_chips_conflict_and_the_congo.json', 'glenn_cantave_the_case_for_ar_and_vr_in_activism.json', 'gunjan_bhardwaj_how_blockchain_and_ai_can_help_us_decipher_medicine_s_big_data.json', 'hany_farid_the_dangers_of_algorithmic_justice.json', 'hosted_by_chris_hemsworth_and_priyanka_chopra_jonas_countdown_session_5_action_full_session.json', 'hosted_by_jane_fonda_and_xiye_bastida_countdown_session_3_transformation_full_session.json', 'hosted_by_mark_ruffalo_and_don_cheadle_countdown_session_1_urgency_full_session.json', 'hosted_by_prajakta_koli_and_hannah_stocking_countdown_session_4_transformation_full_session.json', 'hy_william_chan_how_refugees_could_be_architects_of_their_own_homes_and_futures.json', 'inhi_cho_suh_how_to_achieve_more_quality_hospital_care_with_less_noise.json', 'jack_levis_the_hardest_step_in_innovation_looking_foolish_in_front_of_the_crowd.json', 'jay_cousins_the_future_of_economic_success_is_collaborative.json', 'jennifer_holmgren_how_we_re_turning_carbon_waste_into_jet_fuel_and_everything_else.json', 'jerry_chow_the_future_of_supercomputers_a_quantum_chip_colder_than_outer_space.json', 'jesse_schell_when_games_invade_real_life.json', 'jessica_donohue_the_upside_of_data.json', 'jinha_lee_a_holographic_meeting_platform_for_collaborating_from_anywhere.json', 'joachim_horn_an_easy_way_to_cook_up_innovation.json', 'jonathan_koch_a_data_translation_toolkit_that_anyone_can_use.json', 'judy_brewer_why_we_need_a_more_accessible_digital_landscape.json', 'juliane_gallina_a_library_of_minds.json', 'julia_kloiber_let_s_build_better_digital_tools_for_our_cities.json', 'julie_chang_why_we_need_to_design_cameras_for_robots.json', 'kai_fu_lee_and_chen_qiufan_visions_for_the_future.json', 'karalee_close_will_healthcare_embrace_digital_or_will_we_die_waiting.json', 'kate_e_brandt_a_world_without_waste.json', 'katherine_james_how_agri_robotics_will_change_the_food_you_eat.json', 'kathy_kleiman_the_pioneering_women_who_helped_create_modern_computing.json', 'katie_francfort_what_if_we_used_the_problem_as_part_of_the_solution.json', 'kristi_rogers_where_are_our_digital_ads_really_going.json', 'lana_yarosh_how_technology_can_reconnect_parents_and_kids.json', 'laura_caccia_the_avant_garde_frontier_of_code_poetry.json', 'laura_shin_demystifying_the_wild_world_of_crypto.json', 'lisa_seacat_deluca_a_vision_of_the_future_from_ibm_s_most_prolific_inventor.json', 'lloyd_treinish_this_weather_forecasting_model_is_actually_accurate.json', 'luci_englert_mckean_how_video_calls_can_help_foster_connection.json', 'margaret_morris_the_new_sharing_of_emotions.json', 'mariana_lin_how_we_can_bring_ai_personalities_to_life.json', 'maria_dubovitskaya_take_back_control_of_your_personal_data.json', 'marie_wallace_the_ethics_of_collecting_data.json', 'mario_paluzzi_what_if_we_could_address_the_unaddressable.json', 'marko_russiver_a_global_hackathon_to_take_on_the_coronavirus_pandemic.json', 'markus_lorenz_industry_4_0_how_intelligent_machines_will_transform_everything_we_know.json', 'mark_halliday_bring_out_your_inner_filmmaker.json', 'mark_jackson_a_personal_story_about_farming_and_the_future_of_agriculture.json', 'martin_reeves_your_strategy_needs_a_strategy.json', 'matthew_shifrin_how_i_helped_make_lego_accessible_to_the_blind.json', 'michael_fey_are_you_technically_fit_to_parent.json', 'michael_mieni_using_aboriginal_traditions_to_teach_tech.json', 'michael_ringel_external_innovation_basics_from_an_r_d_expert.json', 'mike_schwartz_the_potential_of_blockchain.json', 'mira_calix_sound_and_sentiment.json', 'monika_blaumueller_how_we_could_use_big_data_to_forecast_the_next_global_outbreak.json', 'moriba_jah_why_we_need_to_clean_up_our_space_debris.json', 'mother_london_ai_therapy.json', 'nivruti_rai_an_open_source_database_to_create_guardian_angel_ai.json', 'patrick_forth_how_can_companies_continue_to_thrive_in_times_of_change.json', 'paul_franklin_the_art_and_science_of_special_effects.json', 'philipp_gerbert_the_basics_of_ai_for_business.json', 'phil_tetlow_8_steps_to_understanding_information_and_maybe_the_universe.json', 'qi_zhang_an_electrifying_organ_performance.json', 'raj_panjabi_investing_in_health_care_workers_strengthens_communities.json', 'raj_panjabi_what_if_we_digitally_empowered_community_health_workers.json', 'rana_novack_how_we_ll_predict_the_next_refugee_crisis.json', 'ranveer_chandra_data_driven_farming_could_transform_agriculture.json', 'robin_hauser_can_we_protect_ai_from_our_biases.json', 'rochelle_king_the_complex_relationship_between_data_and_design_in_ux.json', 'sadasivan_shankar_designing_materials_one_atom_at_a_time.json', 'sarah_willersdorf_what_brands_can_learn_from_online_dating.json', 'sarvesh_subramanian_an_app_that_predicts_the_impact_of_global_climate_change.json', 'shai_weiss_a_vision_for_sustainability_in_aviation.json', 'shelley_shott_training_teachers_10_million_and_counting.json', 'stefaan_verhulst_using_corporate_data_to_improve_our_lives.json', 'stefan_gross_selbeck_business_model_innovation_beating_yourself_at_your_own_game.json', 'stephen_lawrence_the_future_of_reading_it_s_fast.json', 'steve_brown_how_data_is_driving_the_future_of_fashion.json', 'steve_brown_why_machines_must_make_us_better_humans.json', 'steve_jobs_how_to_live_before_you_die.json', 'tanmay_bakshi_technology_that_tackles_the_teen_suicide_epidemic.json', 'tanya_berger_wolf_how_your_nature_photos_can_help_protect_wild_animals.json', 'tan_le_how_does_the_brain_work_in_everyday_situations.json', 'ted_countdown_global_launch_2020.json', 'ted_countdown_global_livestream_2021.json', 'ted_countdown_ted_countdown_dilemma_series_carbon_credits_session_1.json', 'ted_countdown_ted_countdown_dilemma_series_carbon_credits_session_2.json', 'the_holladay_brothers_to_the_beat_of_light.json', 'tim_exile_finding_music_in_mundanity.json', 'tomer_garzberg_what_happens_when_we_take_humans_out_of_work.json', 'truls_nord_tactile_photographs_that_display_worlds_of_light_shadow_and_mood.json', 'vinith_misra_machines_need_an_algorithm_for_humor_this_is_what_it_looks_like.json', 'vishal_sikka_the_beauty_and_power_of_algorithms.json', 'will_cathcart_the_future_of_digital_communication_and_privacy.json', 'yale_fox_home_renters_are_powerless_here_s_how_to_fix_that.json', 'yann_lecun_deep_learning_neural_networks_and_the_future_of_ai.json']\n"
     ]
    }
   ],
   "source": [
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57939b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1054c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.8% of talks do not have a transcript\n"
     ]
    }
   ],
   "source": [
    "pct=len(missing)*100/len(slugs)\n",
    "\n",
    "print(f\"{round(pct,1)}% of talks do not have a transcript\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614123a",
   "metadata": {},
   "source": [
    "> Next: to parse the json files and create the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c785bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
