{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dafc54d8",
   "metadata": {},
   "source": [
    "# TED web scraper\n",
    "\n",
    "The purpose of this notebook is to create a web scraper that collects TED talks:\n",
    "- slugs (url part) by topic\n",
    "- info (talk and speaker details, including transcripts) by slugs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab7afa6",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Downloding slug(s) by topic\n",
    "slug = url element to a talk\n",
    "\n",
    "There are options available:<br>\n",
    "A. **Version 1** - using api request<br> \n",
    "B. **Version 2** - parsing html using beautiful soup -> might not work everytime<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1708cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Inputs\n",
    "\n",
    "topic='technology' # Select a topic from the list available here: https://www.ted.com/topics\n",
    "filename=f\"slug/TED_Talk_{topic}_URLs.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbbcba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1 - using api request\n",
    "\n",
    "def api_scraping(topic, filename):\n",
    "    urls = []\n",
    "    page_number=0\n",
    "    total_pages=0\n",
    "    f = open(filename, 'w')\n",
    "    \n",
    "    while True:\n",
    "        # The API endpoint to request\n",
    "        endpoint = \"https://zenith-prod-alt.ted.com/api/search\"\n",
    "\n",
    "        # The headers for the request\n",
    "        headers = {\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "      }\n",
    "\n",
    "      # The data for the request\n",
    "        data = [{\n",
    "              \"indexName\": \"coyote_models_acme_videos_alias_21e1372f285984be956cd03b7ad3406e\",\n",
    "              \"params\": {\n",
    "                  \"attributeForDistinct\": \"objectID\",\n",
    "                  \"distinct\": 1,\n",
    "                  \"facetFilters\": [\n",
    "                      [\n",
    "                          \"tags:\"+topic\n",
    "                      ]\n",
    "                  ],\n",
    "                  \"facets\": [\n",
    "                      \"subtitle_languages\",\n",
    "                      \"tags\"\n",
    "                  ],\n",
    "                  \"highlightPostTag\": \"__/ais-highlight__\",\n",
    "                  \"highlightPreTag\": \"__ais-highlight__\",\n",
    "                  \"hitsPerPage\": 24,\n",
    "                  \"maxValuesPerFacet\": 500,\n",
    "                  \"page\": page_number,\n",
    "                  \"query\": \"\",\n",
    "                  \"tagFilters\": \"\"\n",
    "              }\n",
    "          },\n",
    "          {\n",
    "              \"indexName\": \"coyote_models_acme_videos_alias_21e1372f285984be956cd03b7ad3406e\",\n",
    "              \"params\": {\n",
    "                  \"analytics\": False,\n",
    "                  \"attributeForDistinct\": \"objectID\",\n",
    "                  \"clickAnalytics\": False,\n",
    "                  \"distinct\": 1,\n",
    "                  \"facets\": \"tags\",\n",
    "                  \"highlightPostTag\": \"__/ais-highlight__\",\n",
    "                  \"highlightPreTag\": \"__ais-highlight__\",\n",
    "                  \"hitsPerPage\": 0,\n",
    "                  \"maxValuesPerFacet\": 500,\n",
    "                  \"page\": page_number,\n",
    "                  \"query\": \"\"\n",
    "              }\n",
    "          }]\n",
    "\n",
    "\n",
    "\n",
    "        # Send the POST request and get the response\n",
    "        response = requests.post(endpoint, headers=headers, data=json.dumps(data))\n",
    "        if page_number==0:\n",
    "            total_pages=response.json()['results'][0]['nbPages']\n",
    "\n",
    "        for slug in response.json()['results'][0]['hits']:\n",
    "            urls.append(slug['slug'])\n",
    "        print(f\"{page_number} / {total_pages}\")\n",
    "        page_number+=1\n",
    "        if page_number>total_pages:\n",
    "            break\n",
    "    f.write('\\n'.join(urls))\n",
    "    f.close()\n",
    "\n",
    "    print(f\"Done.{len(urls)} URLs for topic {topic} have been saved in {f}.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "216da8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2 - using beautiful soup parsing\n",
    "\n",
    "def html_scrapping(topic, filename):\n",
    "    urls = []\n",
    "    page_number=1\n",
    "    f = open(filename, 'w')\n",
    "    \n",
    "    while True:\n",
    "        res  =  requests.get(\"https://www.ted.com/talks?page=\"+str(page_number)+\"&sort=newest&topics[]=\"+topic)\n",
    "\n",
    "        soup = BeautifulSoup(res.text)\n",
    "        e=soup.find('div', id='browse-results')\n",
    "\n",
    "        string_end=\"Sorry. We couldn't find a talk quite like that.\" \n",
    "\n",
    "        if e is None:\n",
    "            break\n",
    "        elif e.find('div', class_='h3 m2') is not None:\n",
    "            if e.find('div', class_='h3 m2').text==string_end:\n",
    "                break\n",
    "        else:\n",
    "            for elem in e.find_all(\"h4\", class_=\"h9 m5 f-w:700\"):\n",
    "                urls.append(elem.find('a')['href'])\n",
    "            page_number+=1\n",
    "\n",
    "\n",
    "    f.write('\\n'.join(urls))\n",
    "    f.close()\n",
    "\n",
    "# try version 2 if this doesn't work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "127d237a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 57\n",
      "1 / 57\n",
      "2 / 57\n",
      "3 / 57\n",
      "4 / 57\n",
      "5 / 57\n",
      "6 / 57\n",
      "7 / 57\n",
      "8 / 57\n",
      "9 / 57\n",
      "10 / 57\n",
      "11 / 57\n",
      "12 / 57\n",
      "13 / 57\n",
      "14 / 57\n",
      "15 / 57\n",
      "16 / 57\n",
      "17 / 57\n",
      "18 / 57\n",
      "19 / 57\n",
      "20 / 57\n",
      "21 / 57\n",
      "22 / 57\n",
      "23 / 57\n",
      "24 / 57\n",
      "25 / 57\n",
      "26 / 57\n",
      "27 / 57\n",
      "28 / 57\n",
      "29 / 57\n",
      "30 / 57\n",
      "31 / 57\n",
      "32 / 57\n",
      "33 / 57\n",
      "34 / 57\n",
      "35 / 57\n",
      "36 / 57\n",
      "37 / 57\n",
      "38 / 57\n",
      "39 / 57\n",
      "40 / 57\n",
      "41 / 57\n",
      "42 / 57\n",
      "43 / 57\n",
      "44 / 57\n",
      "45 / 57\n",
      "46 / 57\n",
      "47 / 57\n",
      "48 / 57\n",
      "49 / 57\n",
      "50 / 57\n",
      "51 / 57\n",
      "52 / 57\n",
      "53 / 57\n",
      "54 / 57\n",
      "55 / 57\n",
      "56 / 57\n",
      "57 / 57\n",
      "Done.1346 URLs for topic technology have been saved in <_io.TextIOWrapper name='slug/TED_Talk_technology_URLs.txt' mode='w' encoding='cp1252'>.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "api_scraping(topic,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7086b0d3",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Downloading content for each talk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a5deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "\n",
    "def getBuildID():\n",
    "    response=requests.get(\"https://ted.com\")\n",
    "    buildID=str(response.content).split(\"buildId\\\":\\\"\")[1].split(\"\\\"\")[0]\n",
    "    return buildID\n",
    "\n",
    "def buildDataURL(slug):\n",
    "    daily_id=getBuildID()\n",
    "    base=f\"https://www.ted.com/_next/data/{daily_id}/talks/\"\n",
    "    mid=\".json?slug=\"\n",
    "    url=base+slug+mid+slug\n",
    "    return url\n",
    "\n",
    "def getSlugData(url):\n",
    "    response=requests.get(url)\n",
    "    try:\n",
    "        return response.json()\n",
    "    except ValueError:\n",
    "        print(response)\n",
    "\n",
    "def getTextFromSlug(talkData):\n",
    "    text = \"\"\n",
    "    for paragraph in talkData[\"pageProps\"][\"transcriptData\"][\"translation\"][\"paragraphs\"]:\n",
    "        for cue in paragraph[\"cues\"]:\n",
    "            text+=\" \"+cue[\"text\"]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c2f90e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TED_Talk_Ai_URLs.txt\n",
      "144 slugs found\n",
      "TED_Talk_computers_URLs.txt\n",
      "213 slugs found\n",
      "TED_Talk_Data_URLs.txt\n",
      "207 slugs found\n",
      "TED_Talk_technology_URLs.txt\n",
      "1346 slugs found\n"
     ]
    }
   ],
   "source": [
    "# define which slugs to be loaded\n",
    "\n",
    "path_to_slug = 'slug/'\n",
    "slug_files = [pos_slug for pos_slug in os.listdir(path_to_slug) if pos_slug.endswith('.txt')]\n",
    "\n",
    "slugs_combined=[]\n",
    "\n",
    "for sfile in slug_files:\n",
    "    print(sfile)\n",
    "    with open(os.path.join(path_to_slug, sfile)) as slug_file:\n",
    "        loaded=slug_file.read().splitlines()\n",
    "        print(f\"{len(loaded)} slugs found\")\n",
    "        slugs_combined.append(loaded)\n",
    "\n",
    "# combining all slugs and removing duplicates        \n",
    "slugs = list(set([item for sublist in slugs_combined for item in sublist]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7824add1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1360\n"
     ]
    }
   ],
   "source": [
    "print(len(slugs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fe1ac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# creating a function to download json data for each slug\n",
    "\n",
    "def extract_json_by_slug(slugs):\n",
    "    count=0\n",
    "    max_count=10\n",
    "\n",
    "    data_list=[]\n",
    "    slugs_retry=[]\n",
    "\n",
    "    for slug in slugs:\n",
    "        count+=1\n",
    "        slugURL = buildDataURL(slug)\n",
    "        data_json = getSlugData(slugURL)\n",
    "\n",
    "        if data_json is None: #checking if the respose is null\n",
    "            slugs_retry.append(slug)\n",
    "\n",
    "        else:\n",
    "            json_object = json.dumps(data_json)\n",
    "            with open(f\"jsons/{topic}_{slug[:50]}.json\", \"w\") as outfile:\n",
    "                outfile.write(json_object)\n",
    "\n",
    "        # adding throttling/wait time to stay within rate rate limiting parameters     \n",
    "        if count>max_count:\n",
    "            time.sleep(10)\n",
    "            count=0\n",
    "        else:\n",
    "            time.sleep(3)\n",
    "    return slugs_retry\n",
    "        \n",
    "if len(slugs_retry)!=0:\n",
    "    extract_json_by_slug(slugs_retry)\n",
    "        \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1110e3db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adam_cutler.json\n",
      "andrew_arru.json\n",
      "ayanna_howa.json\n",
      "briana_brow.jsonbigger issue\n",
      "bruno_miche.json\n",
      "computers_achin_bhowmik_interactive.json\n",
      "computers_david_pogue_simplicity_se.jsonbigger issue\n",
      "computers_emmanuel_schanzer_why_is_.json\n",
      "computers_florian_pinel_how_ibm_s_w.json\n",
      "computers_jerry_chow_the_future_of_.json\n",
      "computers_jinha_lee_reach_into_the_.jsonbigger issue\n",
      "computers_kai_fu_lee_and_chen_qiufa.json\n",
      "computers_kathy_kleiman_the_pioneer.json\n",
      "computers_rodrigo_bijou_governments.jsonbigger issue\n",
      "computers_sadasivan_shankar_designi.json\n",
      "computers_steve_brown_why_machines_.json\n",
      "computers_steve_jobs_how_to_live_be.jsonbigger issue\n",
      "dario_gil_t.json\n",
      "drew_silver.json\n",
      "gunjan_bhar.json\n",
      "juliane_gal.json\n",
      "julie_chang.json\n",
      "mariana_lin.json\n",
      "mother_lond.json\n",
      "nivruti_rai.json\n",
      "philipp_ger.json\n",
      "robin_hause.json\n",
      "vinith_misr.json\n",
      "yann_lecun_.json\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# check for issues: null and missing transcripts\n",
    "count=0\n",
    "path_to_json = 'jsons/'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "for jfile in json_files:\n",
    "    with open(os.path.join(path_to_json, jfile)) as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "        if json_text is None:\n",
    "            print(jfile +\"bigger issue\")\n",
    "        elif json_text[\"pageProps\"][\"transcriptData\"][\"translation\"] is None:\n",
    "            count+=1\n",
    "            print(jfile)\n",
    "         \n",
    "\n",
    "print(count)\n",
    "data_init=pd.json_normalize(data_json['pageProps']['videoData'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
